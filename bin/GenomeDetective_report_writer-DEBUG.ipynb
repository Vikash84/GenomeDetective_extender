{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genome Detective report writer script\n",
    "\n",
    "Author: Sam Nooij  \n",
    "Date: 2018-05-18\n",
    "\n",
    "Input:\n",
    "1. parsed XML file (e.g. \"tmp/GenomeDetective_results.csv\") as generated by bin/GenomeDetective_XML_parser.py\n",
    "2. result CSV files as generated by [GenomeDetective](http://www.genomedetective.com/app/typingtool/virus/)\n",
    "\n",
    "Output: table in csv format, providing:\n",
    "- run_id\n",
    "- sample_id\n",
    "- total_reads\n",
    "- low_quality_reads\n",
    "- non_viral_reads\n",
    "- viral_reads \n",
    "- pcr_result\n",
    "- ct_value \n",
    "- ngs_results\n",
    "- coverage%\n",
    "- contigs\n",
    "- number_of_reads\n",
    "- fraction_of_total_reads\n",
    "- percentage_of_total\n",
    "- fraction_of_viral_reads\n",
    "- percentage_of_viral\n",
    "- pcr_ngs_congruence\n",
    "- pcr_ngs_comments\n",
    "- human_virus_reads\n",
    "- plant_virus_reads\n",
    "- phage_reads\n",
    "- other_viral_reads\n",
    "- runtime\n",
    "\n",
    "Required python packages:\n",
    " - pandas\n",
    " \n",
    "For automatic use in snakemake. The corresponding snakemake rule should provide the input:\n",
    " - the parsed XML file (\"tmp/GenomeDetective_results.csv\")\n",
    " - a list of CSV files (their names, as strings; e.g. [ \"1_a_results.csv\", \"1_b_results.csv\" ]\n",
    " - a name for the output (e.g. \"tmp/GenomeDetective-PCR_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required python libraries\n",
    "import pandas as pd         #dataframe and csv export\n",
    "\n",
    "#CSV_FILES = snakemake.input['csv']\n",
    "#PARSED_XML = snakemake.input['parsed_xml']\n",
    "#OUTPUT_FILE = snakemake.output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../results/4_D_results.csv', '../results/5_5061600092_S2_results.csv', '../results/3_1_results.csv', '../results/6_A1_results.csv']\n"
     ]
    }
   ],
   "source": [
    "### code block for developing/debugging only! ###\n",
    "import glob\n",
    "PARSED_XML = \"../tmp/GenomeDetective_results.csv\"\n",
    "CSV_FILES = glob.glob(\"../results/*_results.csv\")\n",
    "\n",
    "print(CSV_FILES)\n",
    "\n",
    "OUTPUT_FILE = \"/data/benchmark/experiments/20180514_RIVM_stool_GenomeDetective/tmp/Test_merged_df.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parser functions\n",
    "def pull_sample_name(filename):\n",
    "    \"\"\"\n",
    "    The sample and run IDs are in the filename, e.g.:\n",
    "    \"3_10_results.xml\"\n",
    "    where the 3 is the run ID, and the 10 is the sample ID\n",
    "    \"\"\"\n",
    "    error_msg = \"\"\"\n",
    "Expected underscores in the filename with the sample ID, e.g.\n",
    "3_1_results.xml\n",
    "Please provide sample names in this format.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert filename.count('_') >= 2, \\\n",
    "        \"%s\" % error_msg\n",
    "        \n",
    "    run_id = filename.split('/')[-1].split('_')[0]\n",
    "    \n",
    "    if filename.count('_') > 2:\n",
    "        sample_id = '_'.join(filename.split('/')[-1].split('_')[1:-1])\n",
    "    else:\n",
    "        sample_id = filename.split('/')[-1].split('_')[1]\n",
    "    \n",
    "    return run_id, sample_id\n",
    "\n",
    "def create_concatenated_dataframe(csv_list):\n",
    "    \"\"\"\n",
    "    Input: a list of Genome Detective output CSV files,\n",
    "          e.g. [\"3_1_results.csv\", \"4_D_results.csv\"]\n",
    "    Output: One concatenated dataframe of all the input files\n",
    "    \"\"\"\n",
    "    csv_list = sorted(csv_list)\n",
    "    \n",
    "        \n",
    "    #Step 2: open the files as dataframe, remove \"Contigs\" column and add sample IDs\n",
    "    df_list = []\n",
    "    for results_file in csv_list:\n",
    "        results_df = pd.read_csv(results_file)\n",
    "        results_df = results_df.drop(\"Contigs\", axis = 1) #remove unnecessary (and long!) column\n",
    "        run_id, sample_id = pull_sample_name(results_file)\n",
    "        results_df[\"run_id\"] = run_id\n",
    "        results_df[\"sample_id\"] = sample_id\n",
    "        df_list.append(results_df)\n",
    "\n",
    "    #Step 3: concatenate the dataframes\n",
    "    super_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    return(super_df)\n",
    "\n",
    "def combine_tables(parsed_xml, csv_list):\n",
    "    \"\"\"\n",
    "    Input: 1. parsed XML table, with the fields:\n",
    "    run_id sample_id total_reads low_quality_reads non_viral_reads viral_reads runtime\n",
    "           2. CSV results files, with the fields:\n",
    "    Assignment # Contigs Mapped # Reads Coverage (%) Mapped depth <br/>of Coverage NT Identity (%) AA Identity (%) Contigs\n",
    "    Output: one table with the fields:\n",
    "    run_id sample_id total_reads low_quality_reads non_viral_reads viral_reads  pcr_result ct_value ngs_results coverage% contigs number_of_reads fraction_of_total_reads fraction_of_viral_reads pcr_ngs_congruence pcr_ngs_comments human_virus_reads plant_virus_reads phage_reads other_viral_reads runtime\n",
    "    \"\"\"\n",
    "    xml_df = pd.read_csv(parsed_xml)\n",
    "    csv_df = create_concatenated_dataframe(csv_list)\n",
    "    csv_df[\"run_id\"] = csv_df[\"run_id\"].apply(int)\n",
    "    #This column was read as strings and could not merge with\n",
    "    # xml_df, because that contained integers...\n",
    "        \n",
    "    desired_columns = \\\n",
    "    {\"run_id\": \"a\",\"sample_id\": \"a\",\"total_reads\": \"a\",\n",
    "     \"low_quality_reads\": \"a\",\"non_viral_reads\": \"a\",\n",
    "     \"viral_reads\": \"a\",\"pcr_result\": \"m\", \"ct_value\": \"m\",\n",
    "     \"GD_assignment\": \"a\",\"coverage%\": \"a\",\n",
    "     \"contigs\": \"a\",\"number_of_reads\": \"a\",\n",
    "     \"fraction_of_total_reads\": \"c\",\"fraction_of_viral_reads\": \"c\",\n",
    "     \"pcr_ngs_congruence\": \"m\", \"pcr_ngs_comments\": \"m\",\n",
    "     \"human_virus_reads\": \"m\", \"plant_virus_reads\": \"m\",\n",
    "     \"phage_reads\": \"m\", \"other_viral_reads\": \"m\",\"runtime\": \"a\"}\n",
    "    #a = available data (can copy), m = manual filling, c = calculate\n",
    "\n",
    "    original_columns = \\\n",
    "    {\n",
    "        \"GD_assignment\": \"Assignment\", \"contigs\": \"# Contigs\",\n",
    "        \"number_of_reads\": \"Mapped # Reads\", \"coverage%\": \"Coverage (%)\",\n",
    "    }\n",
    "    report_df = xml_df\n",
    "    empty = \"fill me in\"\n",
    "    \n",
    "    #First step: merge the 'a' part\n",
    "    report_df = report_df.merge(csv_df, how = \"right\", on = [\"run_id\", \"sample_id\"])\n",
    "\n",
    "    #Second step: fill in the gaps\n",
    "    for key, value in desired_columns.items():\n",
    "        if value == \"m\":\n",
    "        #These have to be filled manually\n",
    "            report_df[key] = empty\n",
    "\n",
    "        elif value == \"a\":\n",
    "        #These data are avaible and automatically filled in\n",
    "            if key in report_df:\n",
    "                pass\n",
    "            else:\n",
    "                #check if the column name is different\n",
    "                if key in original_columns:\n",
    "                    key = original_columns[key]\n",
    "                #report it missing if it is still not there\n",
    "                if key not in report_df:\n",
    "                    print(\"%s is a missing column\" % key)\n",
    "                #If it is in the dataframe already, pass\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "        elif value == \"c\":\n",
    "        #These data have to be calculated with what is available\n",
    "            if \"total\" in key:\n",
    "                report_df = report_df.assign(\n",
    "                    fraction_of_total_reads=lambda report_df:\n",
    "                    report_df[\"Mapped # Reads\"] / report_df.total_reads)\n",
    "                #fraction = number / total_reads\n",
    "                report_df = report_df.assign(\n",
    "                    percentage_of_total=lambda report_df:\n",
    "                    report_df.fraction_of_total_reads * 100)\n",
    "                #percentage = fraction * 100\n",
    "            elif \"viral\" in key:\n",
    "                report_df = report_df.assign(\n",
    "                    fraction_of_viral_reads=lambda report_df:\n",
    "                    report_df[\"Mapped # Reads\"] / report_df.viral_reads)\n",
    "                #fraction = number / viral_reads\n",
    "                report_df = report_df.assign(\n",
    "                    percentage_of_viral=lambda report_df:\n",
    "                    report_df.fraction_of_viral_reads * 100)\n",
    "                #percentage = fraction * 100\n",
    "                pass\n",
    "            \n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    report_df = report_df.drop(\"Mapped depth <br/>of Coverage\", axis = 1)\n",
    "    \n",
    "    for key, value in original_columns.items():\n",
    "        report_df = report_df.rename(columns={value: key})\n",
    "\n",
    "    return report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File ../tmp/GenomeDetective_results.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-2b32312e7e3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#Parse/collect the results in a Pandas dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mreport_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine_tables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_xml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPARSED_XML\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCSV_FILES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#Reorder the columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-5f27d34928e9>\u001b[0m in \u001b[0;36mcombine_tables\u001b[0;34m(parsed_xml, csv_list)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mrun_id\u001b[0m \u001b[0msample_id\u001b[0m \u001b[0mtotal_reads\u001b[0m \u001b[0mlow_quality_reads\u001b[0m \u001b[0mnon_viral_reads\u001b[0m \u001b[0mviral_reads\u001b[0m  \u001b[0mpcr_result\u001b[0m \u001b[0mct_value\u001b[0m \u001b[0mngs_results\u001b[0m \u001b[0mcoverage\u001b[0m\u001b[0;34m%\u001b[0m \u001b[0mcontigs\u001b[0m \u001b[0mnumber_of_reads\u001b[0m \u001b[0mfraction_of_total_reads\u001b[0m \u001b[0mfraction_of_viral_reads\u001b[0m \u001b[0mpcr_ngs_congruence\u001b[0m \u001b[0mpcr_ngs_comments\u001b[0m \u001b[0mhuman_virus_reads\u001b[0m \u001b[0mplant_virus_reads\u001b[0m \u001b[0mphage_reads\u001b[0m \u001b[0mother_viral_reads\u001b[0m \u001b[0mruntime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \"\"\"\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mxml_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_xml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mcsv_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_concatenated_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mcsv_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/python/anaconda2/envs/panbio/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/python/anaconda2/envs/panbio/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/python/anaconda2/envs/panbio/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/python/anaconda2/envs/panbio/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software/python/anaconda2/envs/panbio/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1211\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3427)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:6861)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File ../tmp/GenomeDetective_results.csv does not exist"
     ]
    }
   ],
   "source": [
    "#Script execution\n",
    "if __name__ == \"__main__\":\n",
    "    #Parse/collect the results in a Pandas dataframe\n",
    "    report_df = combine_tables(parsed_xml = PARSED_XML, csv_list = CSV_FILES)\n",
    "    \n",
    "    #Reorder the columns\n",
    "    column_order = [\"run_id\", \"sample_id\",\n",
    "                    \"pcr_result\", \"ct_value\", \n",
    "                    \"GD_assignment\", \"coverage%\", \"contigs\", \n",
    "                    \"pcr_ngs_congruence\", \"pcr_ngs_comments\",\n",
    "                    \"number_of_reads\", \"fraction_of_total_reads\", \n",
    "                    \"percentage_of_total\", \"fraction_of_viral_reads\", \n",
    "                    \"percentage_of_viral\",\n",
    "                    \"total_reads\", \"low_quality_reads\", \n",
    "                    \"non_viral_reads\", \"viral_reads\",\n",
    "                    \"human_virus_reads\", \"plant_virus_reads\",\n",
    "                    \"phage_reads\", \"other_viral_reads\", \"runtime\"]\n",
    "    \n",
    "    report_df = report_df[column_order]\n",
    "    \n",
    "    #And save it as a csv file\n",
    "    report_df.to_csv(OUTPUT_FILE, index = False)\n",
    "    \n",
    "    print(\"\"\"\\nDone!\n",
    "The results have been written to: %s\"\"\" % OUTPUT_FILE)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
