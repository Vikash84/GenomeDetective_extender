{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genome Detective XML parser script\n",
    "\n",
    "Author: Sam Nooij  \n",
    "Date: 2018-05-18\n",
    "\n",
    "Input: results.xml file as generated by [Genome Detective](http://www.genomedetective.com/app/typingtool/virus/)\n",
    "\n",
    "Output: table in csv format, providing:\n",
    "\n",
    "| run_id | sample_id | total_reads | low_quality_reads | non_viral_reads | viral_reads | runtime | \n",
    "| ------ | --------- | ----------- | ----------------- | --------------- | ----------- | ------ |\n",
    "| ...    | ...       | ...         | ...               | ...             | ...         | ...    |\n",
    "\n",
    "Required python packages:\n",
    " - lxml\n",
    " - icu\n",
    " - pandas\n",
    " \n",
    "For automatic use in snakemake. The corresponding snakemake rule should provide the input:\n",
    " - a list of XML files (their names, as strings; e.g. [ \"1_a_results.xml\", \"1_b_results.xml\" ]\n",
    " - a name for the output (e.g. \"tmp/GenomeDetective_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import requires python libraries\n",
    "from lxml import etree      #XML parser\n",
    "import pandas as pd         #dataframe and csv export\n",
    "from collections import defaultdict\n",
    "\n",
    "#XML_FILES = snakemake.input[0]\n",
    "#OUTPUT_FILE = snakemake.output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../results/4_D_results.xml', '../results/5_5061600092_S2_results.xml', '../results/3_1_results.xml', '../results/6_A1_results.xml']\n"
     ]
    }
   ],
   "source": [
    "### code block for developing/debugging only! ###\n",
    "import glob\n",
    "XML_FILES = glob.glob(\"../results/*_results.xml\")\n",
    "print(XML_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing files: ['../results/4_D_results.xml', '../results/5_5061600092_S2_results.xml', '../results/3_1_results.xml', '../results/6_A1_results.xml']\n",
      "Now analysing sample: ../results/4_D_results.xml\n",
      "Now analysing sample: ../results/5_5061600092_S2_results.xml\n",
      "Now analysing sample: ../results/3_1_results.xml\n",
      "Now analysing sample: ../results/6_A1_results.xml\n",
      "   low_quality_reads  non_viral_reads run_id  runtime      sample_id  \\\n",
      "0              81660           702776      4     1958              D   \n",
      "1             791514         10163182      5     7980  5061600092_S2   \n",
      "2              62010           203950      3     7821              1   \n",
      "3             168586           470322      6     7734             A1   \n",
      "\n",
      "   total_reads  viral_reads  \n",
      "0       871250        86814  \n",
      "1     16289466      5334770  \n",
      "2       849294       583334  \n",
      "3      2011292      1372384  \n"
     ]
    }
   ],
   "source": [
    "#Parser functions\n",
    "def pull_sample_name(filename):\n",
    "    \"\"\"\n",
    "    The sample and run IDs are in the filename, e.g.:\n",
    "    \"3_10_results.xml\"\n",
    "    where the 3 is the run ID, and the 10 is the sample ID\n",
    "    \"\"\"\n",
    "    error_msg = \"\"\"\n",
    "Expected underscores in the filename with the sample ID, e.g.\n",
    "3_1_results.xml\n",
    "Please provide sample names in this format.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert filename.count('_') >= 2, \\\n",
    "        \"%s\" % error_msg\n",
    "        \n",
    "    run_id = filename.split('/')[-1].split('_')[0]\n",
    "    \n",
    "    if filename.count('_') > 2:\n",
    "        sample_id = '_'.join(filename.split('/')[-1].split('_')[1:-1])\n",
    "    else:\n",
    "        sample_id = filename.split('/')[-1].split('_')[1]\n",
    "    \n",
    "    return run_id, sample_id\n",
    "\n",
    "def parse_xml(filename):\n",
    "    \"\"\"\n",
    "    parse Genome Detective XML output files\n",
    "    with help from:\n",
    "    http://lxml.de/parsing.html#iterparse-and-iterwalk\n",
    "    \n",
    "    collects for each sample:\n",
    "     - total number of reads\n",
    "     - number of low quality reads (qc-removed)\n",
    "     - number of viral reads (as identified by DIAMOND)\n",
    "     - number of non-viral reads (as identified by DIAMOND)\n",
    "     - total runtime (in milliseconds -> converted to seconds)\n",
    "    \"\"\"\n",
    "    init = True         #for finding start_time\n",
    "    finished = False    #for finding end_time\n",
    "    qc1 = False         #for finding total_reads\n",
    "    qc2 = False         #for filding low_quality_reads\n",
    "    filtering = False   #for finding viral & non-viral reads\n",
    "    viral = False       #for counting viral reads\n",
    "    \n",
    "    viral_reads = 0\n",
    "    #sum all reads with \"Viruses\" in their taxonomy, starting from 0\n",
    "        \n",
    "    context = etree.iterparse(filename)\n",
    "    \n",
    "    for action, elem in context:\n",
    "        #Always check for these keywords:\n",
    "        if elem.tag == \"init\":\n",
    "        #Indicates the **end** of the init block!\n",
    "        # - By default, only the ends of blocks are parsed:\n",
    "        #   this way I prevent reading elements twice\n",
    "            init = False\n",
    "            qc1 = True\n",
    "        elif elem.tag == \"qc1\":\n",
    "            qc1 = False\n",
    "            qc2 = True\n",
    "        elif elem.tag == \"qc2\":\n",
    "            qc2 = False\n",
    "            filtering = True\n",
    "        elif elem.tag == \"filtering\":\n",
    "            filtering = False\n",
    "        elif elem.tag == \"consensus-read-count\":\n",
    "            finished = True\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        #Search for start_time\n",
    "        if init and elem.tag == \"start-time\":\n",
    "            start_time = int(elem.text)\n",
    "            \n",
    "        #Search for total_reads\n",
    "        if qc1 and elem.tag == \"read-count\":\n",
    "            total_reads = int(elem.text)\n",
    "        \n",
    "        #Search for high_quality_reads, to calculate the\n",
    "        # low_quality_reads\n",
    "        if qc2 and elem.tag == \"read-count\":\n",
    "            high_quality_reads = int(elem.text)\n",
    "        \n",
    "        #Search for viral reads\n",
    "        if filtering:\n",
    "            #In the XML format, viruses can be recognized by\n",
    "            #\"Viruses\" in the \"ancestors\" field (= higher \n",
    "            # taxonomic levels).\n",
    "            if elem.tag == \"ancestors\":\n",
    "                if \"Viruses\" in elem.text:\n",
    "                    viral = True\n",
    "                else:\n",
    "                    viral = False\n",
    "            elif elem.tag == \"end-time\":\n",
    "                viral == False\n",
    "            else:\n",
    "                pass\n",
    "            if viral:\n",
    "                if elem.tag == \"read-count-total\":\n",
    "                    viral_reads += int(elem.text)\n",
    "                    #I take the sum of all reads in viral \n",
    "                    # buckets.\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        #Search for end_time\n",
    "        if finished and elem.tag == \"end-time\":\n",
    "            end_time = int(elem.text)\n",
    "    \n",
    "    #Runtimes are reported in ms: convert to seconds\n",
    "    runtime = ( end_time - start_time ) / 1000\n",
    "    \n",
    "    low_quality_reads = total_reads - high_quality_reads\n",
    "    \n",
    "    non_viral_reads = total_reads - low_quality_reads - viral_reads\n",
    "    \n",
    "    return {\"total_reads\" : total_reads,\n",
    "            \"low_quality_reads\" : low_quality_reads, \n",
    "            \"non_viral_reads\" : non_viral_reads, \n",
    "            \"viral_reads\" : viral_reads, \n",
    "            \"runtime\" : runtime}\n",
    "\n",
    "def aggregate_results(file_list):\n",
    "    \"\"\"\n",
    "    Collects results for each sample/file and puts all in a\n",
    "    single pandas dataframe object.\n",
    "    Input: a list of files\n",
    "    Output: a pandas dataframe\n",
    "    \"\"\"\n",
    "    #this works easiest if the dictionary contains a list of\n",
    "    # result for each value, e.g.:\n",
    "    # {\"run_id\" : [\"run3\", \"run4\", \"run5\"]}\n",
    "    #I.e. don't make every dictionary separately, but combine\n",
    "    # all results into a single one!\n",
    "    \n",
    "    #Tip: search for \"merging dictionaries while keeping the original values\"\n",
    "    #See: https://stackoverflow.com/questions/36180688/merging-two-dictionaries-while-keeping-the-original#36180760\n",
    "    # and: https://code.tutsplus.com/tutorials/how-to-merge-two-python-dictionaries--cms-26230\n",
    "    \n",
    "    #Collect the results and put them in a dictionary.\n",
    "    results_dict = defaultdict(list)\n",
    "    \n",
    "    for sample in file_list:\n",
    "        print(\"Now analysing sample: %s\" % sample)\n",
    "        run_id, sample_id = pull_sample_name(sample)\n",
    "        xml_data = parse_xml(sample)\n",
    "        \n",
    "        xml_data[\"run_id\"] = run_id\n",
    "        xml_data[\"sample_id\"] = sample_id\n",
    "        \n",
    "        #append the results to the 'overall' results dict\n",
    "        for key, value in xml_data.items():\n",
    "            results_dict[key].append(value)\n",
    "\n",
    "    #And convert the results to a dataframe\n",
    "    results_df = pd.DataFrame(results_dict)\n",
    "            \n",
    "    return(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided files: ['4_D_results.xml', '5_5061600092_S2_results.xml', '3_1_results.xml', '6_A1_results.xml']\n",
      "\n",
      "Now analysing sample: ../results/4_D_results.xml\n",
      "Now analysing sample: ../results/5_5061600092_S2_results.xml\n",
      "Now analysing sample: ../results/3_1_results.xml\n",
      "Now analysing sample: ../results/6_A1_results.xml\n",
      "\n",
      "Done!\n",
      "The results have been written to: GenomeDetective_results.csv\n"
     ]
    }
   ],
   "source": [
    "#Script execution\n",
    "if __name__ == \"__main__\":\n",
    "    #Show which files are being analysed\n",
    "    print(\"Provided files: %s\\n\" % \n",
    "          ([file.split('/')[-1] for file in XML_FILES]))\n",
    "    \n",
    "    #Parse/collect the results in a Pandas dataframe\n",
    "    results_df = aggregate_results(XML_FILES)\n",
    "    \n",
    "    #Reorder the columns\n",
    "    columns = [\"run_id\", \"sample_id\", \"total_reads\", \n",
    "               \"low_quality_reads\", \"non_viral_reads\",\n",
    "               \"viral_reads\", \"runtime\"]\n",
    "    results_df = results_df[columns]\n",
    "    \n",
    "    #And save it as a csv file\n",
    "    results_df.to_csv(\"GenomeDetective_results.csv\", \n",
    "                      index = False)\n",
    "    #results_df.to_csv(OUTPUT_FILE, index = False)\n",
    "    \n",
    "    print(\"\"\"\\nDone!\n",
    "The results have been written to: GenomeDetective_results.csv\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
